{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections # For frequency counting\n",
    "import findspark\n",
    "findspark.init(\"/home/alex/learn-spark-python/spark2\")\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import DataFrameNaFunctions\n",
    "from pyspark.sql.functions import lit # Create columns of *literal* value\n",
    "from pyspark.sql.functions import col # Returns a Column based on the \n",
    "                                      # given column name\n",
    "from pyspark.ml.feature import StringIndexer #label encoding\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"helloworld\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDF = spark.read.csv(\"/home/alex/learn-spark-python/data/titanic/train.csv\", header=\"true\")\n",
    "testDF = spark.read.csv(\"/home/alex/learn-spark-python/data/titanic/test.csv\", header=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine train and test data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDF.select(['Embarked']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDF.select(['Embarked']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDF = trainDF.withColumn('Mark', lit('train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.select(['Mark']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Add Survived column to test, and dataset name as a column\n",
    "testDF = (testDF.withColumn('Survived',lit(0))\n",
    "                .withColumn('Mark', lit('test')))\n",
    "testDF = testDF[trainDF.columns]\n",
    "\n",
    "## Append Test data to Train data\n",
    "df = trainDF.unionAll(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps in a Machine Learning Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data Collection\n",
    "* Data Preprocessing\n",
    "* Feature Engineering\n",
    "* Data format translation\n",
    "* Modeling\n",
    "* Evaluation and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Combiniing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Statistical Summary\n",
    "* Histograms\n",
    "* Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the schema?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which ones are numeric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the variables which should be numeric (float or integer):\n",
    "\n",
    "* PassengerId: Integer\n",
    "* Pclass: Integer\n",
    "* SibSp: Integer\n",
    "* Parch: Integer\n",
    "* Survived: Integer\n",
    "* Age: Float\n",
    "* Fare: Float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here is an example\n",
    "df = df.withColumn(\"AgeTmp\", df[\"Age\"].cast(\"float\")) \\\n",
    "    .drop(\"Age\") \\\n",
    "    .withColumnRenamed(\"AgeTmp\", \"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define function\n",
    "def to_anytype(df, colnames, typename):\n",
    "    for colname in colnames:\n",
    "        df = df.withColumn(\"tmp\", df[colname].cast(typename)) \\\n",
    "        .drop(colname) \\\n",
    "        .withColumnRenamed(\"tmp\", colname)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intCols = ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Survived']\n",
    "floatCols = ['Age', 'Fare']\n",
    "\n",
    "df = to_anytype(df, intCols, \"integer\")\n",
    "df = to_anytype(df, floatCols, \"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe('Age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.describe(['Age', 'Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.describe(trainDF.columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.describe(trainDF.columns[1:4]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.describe(trainDF.columns[5:8]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.describe(trainDF.columns[9:12]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We need the frequency count of various levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_hist = spark.sql(\n",
    "    \"SELECT Age AS age, \\\n",
    "            count(*) AS count \\\n",
    "    FROM train \\\n",
    "    GROUP BY Age \\\n",
    "    ORDER BY Age\")\n",
    "age_hist.show(n=age_hist.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_hist = spark.sql(\n",
    "    \"SELECT bucket_floor, \\\n",
    "        CONCAT(bucket_floor, ' to ', bucket_ceiling) as bucket_name, \\\n",
    "        count(*) as count \\\n",
    "     FROM ( \\\n",
    "        SELECT floor(Age/5.00)*5 as bucket_floor, \\\n",
    "            floor(Age/5.00)*5 + 5 as bucket_ceiling \\\n",
    "        FROM train \\\n",
    "     ) a \\\n",
    "     GROUP BY 1, 2 \\\n",
    "     ORDER BY 1\")\n",
    "\n",
    "age_hist.show(n=age_hist.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_column(df, colname):\n",
    "    coldata = df.rdd.map(lambda r: r[colname]).collect()\n",
    "    coldata = ['None' if v is None else v for v in coldata] #replace None values\n",
    "    return(coldata)\n",
    "\n",
    "age = get_column(age_hist, \"bucket_name\")\n",
    "count = get_column(age_hist, \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "barplt = sns.barplot(age, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "barplt = sns.barplot(age, count)\n",
    "for item in barplt.get_xticklabels():\n",
    "    item.set_rotation(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_column(df, colname):\n",
    "    coldata = df.rdd.map(lambda r: r[colname]).collect()\n",
    "    coldata = ['None' if v is None else v for v in coldata] #replace None values\n",
    "    return(coldata)\n",
    "\n",
    "def histplot(dfname, colname, binsize):\n",
    "    binsize = str(binsize)\n",
    "    dfname.createOrReplaceTempView(\"tmpDF\")\n",
    "    hist_query = \"SELECT bucket_floor, \\\n",
    "        CONCAT(bucket_floor, ' to ', bucket_ceiling) as bucket_name, \\\n",
    "        count(*) as count \\\n",
    "     FROM ( \\\n",
    "        SELECT floor(\" + colname + \"/\" + binsize + \")*\" + binsize + \" as bucket_floor, \\\n",
    "            floor(\" + colname + \"/\" + binsize + \")*\" + binsize + \" + \" + binsize + \" as bucket_ceiling \\\n",
    "        FROM tmpDF \\\n",
    "     ) a \\\n",
    "     GROUP BY 1, 2 \\\n",
    "     ORDER BY 1\"\n",
    "    hist_data = spark.sql(hist_query)\n",
    "    xvar = get_column(hist_data, \"bucket_name\")\n",
    "    count = get_column(hist_data, \"count\")\n",
    "    barplt = sns.barplot(xvar, count)\n",
    "    for item in barplt.get_xticklabels():\n",
    "        item.set_rotation(45)\n",
    "    return(barplt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histplot(df, \"Age\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histplot(df, \"Age\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Play with various binsizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "histplot(df, \"Age\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "histplot(df, \"Survived\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "histplot(df, \"Survived\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "histplot(df, \"Pclass\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "histplot(df, \"SibSp\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "histplot(df, \"Parch\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "histplot(df, \"Age\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "histplot(df, \"Fare\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "histplot(df, \"Fare\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test with a categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "histplot(trainDF, \"Embarked\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def histplot_s(df, colname):\n",
    "    xvar = get_column(df, colname)\n",
    "    counter = collections.Counter(xvar)\n",
    "    barplt = sns.barplot(list(counter.keys()), list(counter.values()))\n",
    "    for item in barplt.get_xticklabels():\n",
    "        item.set_rotation(45)\n",
    "    return(barplt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histplot_s(df, \"Sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histplot_s(df, \"Embarked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.corr(\"Age\", \"Fare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.corr(\"Age\", \"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.corr(\"Fare\", \"Survived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, only *pearson* is supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numVars = ['Survived','Age','SibSp','Parch','Fare']\n",
    "stringVars = ['Cabin', 'Embarked', 'Pclass', 'Sex']\n",
    "\n",
    "def countNull(df,var):\n",
    "    return df.where(df[var].isNull()).count()\n",
    "\n",
    "def countEmptyString(df,var):\n",
    "    return df[df[var].isin(\"\")].count()\n",
    "\n",
    "def countZero(df,var):\n",
    "    return df[df[var].isin(0)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = {var: countNull(df,var) for var in df.columns}\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = {var: countEmptyString(df, var) for var in df.columns}\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = {var: countZero(df, var) for var in df.columns}\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_mean = df.groupBy().mean('Age').first()\n",
    "age_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "age_mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_mean = df.groupBy().mean('Age').first()[0]\n",
    "fare_mean = df.groupBy().mean('Fare').first()[0]\n",
    "age_mean, fare_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.na.fill({'Age':age_mean,'Fare':fare_mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is wrong with what I just did?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Univariate\n",
    "    - Winsorization\n",
    "* Multivariate\n",
    "\n",
    "* Is it a good idea?\n",
    "* Know your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Domain Expertise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    " \n",
    "## create user defined function to extract title\n",
    "getTitle = udf(lambda name: name.split('.')[0].strip(), StringType())\n",
    "df = df.withColumn('Title', getTitle(df['Name']))\n",
    " \n",
    "df.select('Name','Title').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTitle = udf(lambda name: name.split('.')[0].split(',')[1].strip(), StringType())\n",
    "df = df.withColumn('Title', getTitle(df['Name']))\n",
    " \n",
    "df.select('Name','Title').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variable treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some algorithms can handle categorical variables directly, some can't.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Label Encoder\n",
    "    - It is used to transform non-numerical labels to numerical labels (or nominal categorical variables)\n",
    "    - Numerical labels are always between 0 and n_classes-1\n",
    "    - May introduce spurious relationship\n",
    "        * Age and City\n",
    "* One Hot Encoding\n",
    "    - Encodes categorical integer features using a one-hot aka one-of-K scheme\n",
    "    - Preferable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding (Indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "catVars = ['Pclass','Sex','Embarked','Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## index Sex variable\n",
    "si = StringIndexer(inputCol = 'Sex', outputCol = 'Sex_indexed')\n",
    "df_indexed = si.fit(df).transform(df).drop('Sex').withColumnRenamed('Sex_indexed','Sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make use of pipeline to index all categorical variables\n",
    "def indexer(df, col):\n",
    "    si = StringIndexer(inputCol = col, outputCol = col+'_indexed').fit(df)\n",
    "    return si\n",
    " \n",
    "indexers = [indexer(df, col) for col in catVars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = indexers)\n",
    "df_indexed = pipeline.fit(df).transform(df)\n",
    "df_indexed.select('Embarked','Embarked_indexed').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = indexers)\n",
    "df_indexed = pipeline.fit(df_indexed).transform(df_indexed)\n",
    " \n",
    "df_indexed.select('Embarked','Embarked_indexed').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The categorical features are indexed in resulting data\n",
    "* Embarked is mapped S=>0, C=>1, Q=>2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StringIndexer\n",
    "\n",
    "* Maps a string column of labels to a column of label indices\n",
    "* If the input column is numeric, we cast it to string and index the string values\n",
    "* The indices are in [0, numLabels), ordered by label frequencies\n",
    "    - So the most frequent label gets index 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer\n",
    "\n",
    "* transform one dataset into another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimator\n",
    "\n",
    "* fit models to data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipelines\n",
    "\n",
    " \n",
    "* A Pipeline consists of a sequence of stages, each of which is either an Estimator or a Transformer\n",
    "* When Pipeline.fit() is called, the stages are executed in order\n",
    "    - If a stage is an Estimator, its Estimator.fit() method will be called on the input dataset to fit a model\n",
    "        * Then the model, which is a transformer, will be used to transform the dataset as the input to the next stage\n",
    "    - If a stage is a Transformer, its Transformer.transform() method will be called to produce the dataset for the next stage\n",
    "* The fitted model from a Pipeline is a PipelineModel, which consists of fitted models and transformers, corresponding to the pipeline stages\n",
    "* If there are no stages, the pipeline acts as an identity transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeseries Variable treatments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Shattering\n",
    "* No time/day variables here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data format translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this step, we get the data in the format or data type expected by the algorithms\n",
    "* In the case of Spark MLlib, this includes \n",
    "    - local vector\n",
    "    - dense or sparse vectors\n",
    "    - labeled points\n",
    "    - local matrix\n",
    "    - distributed matrix with row matrix\n",
    "    - indexed row matrix\n",
    "    - coordinate matrix\n",
    "    - block matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we need convert features to Vectors (either SparseVector or DenseVector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import DenseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catVarsIndexed = [i + '_indexed' for i in catVars]\n",
    "catVarsIndexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCol = numVars + catVarsIndexed\n",
    "featuresCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCol.remove('Survived')\n",
    "featuresCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelCol = ['Mark','Survived']\n",
    "labelCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = Row('mark','label','features') \n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexed = df_indexed[labelCol + featuresCol]\n",
    "df_indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0-mark, 1-label, 2-features\n",
    "# map features to DenseVector\n",
    "lf = df_indexed.rdd.map(lambda r: (row(r[0], r[1],DenseVector(r[2:])))).toDF()\n",
    "lf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# index label\n",
    "# convert numeric label to categorical, which is required by\n",
    "# decisionTree and randomForest\n",
    "lf = StringIndexer(inputCol = 'label', outputCol='index').fit(lf).transform(lf)\n",
    " \n",
    "lf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split back into train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = lf.where(lf.mark =='train')\n",
    "test = lf.where(lf.mark =='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random split further to get train/validate\n",
    "train, validate = train.randomSplit([0.7,0.3], seed =121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Train Data Number of Row: '+ str(train.count()))\n",
    "print('Validate Data Number of Row: '+ str(validate.count()))\n",
    "print('Test Data Number of Row: '+ str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ML is built based on DataFrame, while mllib is based on RDD\n",
    "* We'll fit the logistic, decision tree and random forest models from ML packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    " \n",
    "# regPara: lasso regularisation parameter (L1)\n",
    "lr = LogisticRegression(maxIter = 100, regParam = 0.05, labelCol='index').fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model based on auc ROC(default for binary classification)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    " \n",
    "def testModel(model, validate = validate):\n",
    "    pred = model.transform(validate)\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol = 'index')\n",
    "    return evaluator.evaluate(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('AUC ROC of Logistic Regression model is: ' + str(testModel(lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('AUC ROC of Logistic Regression model is: ' + str(testModel(lr, validate=test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_test = lr.transform(test)\n",
    "pred_test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\n",
    " \n",
    "dt = DecisionTreeClassifier(maxDepth = 3, labelCol ='index').fit(train)\n",
    "rf = RandomForestClassifier(numTrees = 100, labelCol = 'index').fit(train)\n",
    "gbt = GBTClassifier(maxIter = 10, labelCol = 'index').fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = {'LogisticRegression':lr,\n",
    "          'DecistionTree':dt,\n",
    "          'RandomForest':rf}\n",
    " \n",
    "modelPerf = {k:testModel(v) for k,v in models.items()}\n",
    "print(modelPerf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_acc(model, validate=validate):\n",
    "    pred = model.transform(validate)\n",
    "    eval_vec = np.array(get_column(pred, \"label\")) == np.array(get_column(pred, \"prediction\")) \n",
    "    return(eval_vec.sum()/len(eval_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_acc(gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = {'LogisticRegression':lr,\n",
    "          'DecistionTree':dt,\n",
    "          'RandomForest':rf,\n",
    "          'GradientBoostingMachines':gbt}\n",
    "\n",
    "modelPerf = {k:model_acc(v) for k,v in models.items()}\n",
    "print(modelPerf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    dt = DecisionTreeClassifier(maxDepth = i, labelCol ='index').fit(train)\n",
    "    print('AUC ROC of Decision Tree model is' + '(for maxDepth= ' + str(i) + '): ' + str(testModel(dt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(5, 200):\n",
    "    rf = RandomForestClassifier(numTrees = i, labelCol = 'index').fit(train)\n",
    "    print('AUC ROC of Random Forest model is' + '(for numTrees= ' + str(i) + '): ' + str(testModel(rf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
